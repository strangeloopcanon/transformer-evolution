arch:
  d_model: 512
  n_layers: 16
  norm: RMSNorm
  mix_unit:
    kind: "single"
    mixer:
      kind: "Attention"
      heads: 16
      groups: 4
      softmax: { qk_norm: "rms" }
  ffn: { kind: "dense", mult: 3.2, act: "swiglu" }
  pos: { kind: "rope", rope: { theta: 10000, dims: 64 } }
  kv_policy:
    cache: "latent"
    latent: { dim: 256, heads: 8, update: { kind: "linear", freq: 8, tau: 0.05 } }
train: { ctx_len: 4096, dtype: fp16 }
