arch:
  d_model: 640
  n_layers: 8
  norm: RMSNorm
  mix_unit:
    kind: "single"
    mixer: { kind: "Attention", heads: 8, groups: 2, stencil: { kind: "full" } }
  ffn: { kind: "dense", mult: 3.5, act: "swiglu" }
  pos: { kind: "rope", rope: { theta: 25000, dims: 128, scaling: { type: "yarn", factor: 1.5 } } }
  modules:
    embeddings:
      kind: "embedding"
      params: { vocab_size: 65536 }
      budget: { tokens_per_step: 65536, flops_per_token: 0.0 }
    encoder_noncausal:
      kind: "transformer"
      d_model: 640
      n_layers: 4
      mix_unit:
        kind: "single"
        mixer: { kind: "Attention", heads: 8, groups: 2, stencil: { kind: "full" }, pos: "rope" }
      ffn: { kind: "dense", mult: 3.0, act: "swiglu" }
      norm: RMSNorm
      pos: { kind: "rope", rope: { theta: 25000, dims: 128 } }
    latent_sampler:
      kind: "latent_sampler"
      cond:
        source: { kind: "pool-mlp", H: 16 }
        reg: { kind: "freebits", kappa: 0.75 }
        ops:
          - { where: "pre_mixer", op: "film", share: "per_channel" }
          - { where: "proj_q", op: "lora", r: 8 }
      params: { latent_dim: 256 }
    decoder_lower:
      kind: "transformer"
      d_model: 640
      n_layers: 2
      mix_unit:
        kind: "route"
        choices:
          - { kind: "Attention", heads: 8, groups: 2, stencil: { kind: "local", window: 1024 } }
          - { kind: "Retention", heads: 8, chunk: 512, mode: "parallel" }
          - { kind: "SSM", d_state: 24, expand: 1.5 }
        router: { topk: 2, temp: 0.6, balance: 0.02 }
        merge: "WeightedAdd"
      ffn: { kind: "dense", mult: 3.5, act: "swiglu" }
      norm: RMSNorm
      pos: { kind: "rope", rope: { theta: 25000, dims: 128 } }
      kv_policy: { cache: "window", window: 6144, quant: "nf4" }
    decoder_upper:
      kind: "transformer"
      d_model: 640
      n_layers: 2
      mix_unit:
        kind: "single"
        mixer: { kind: "Attention", heads: 8, groups: 2, stencil: { kind: "ring", block: 256, stride: 128 }, softmax: { qk_norm: "layer" }, pos: "rope" }
      ffn: { kind: "dense", mult: 3.5, act: "swiglu" }
      norm: RMSNorm
      pos: { kind: "rope", rope: { theta: 25000, dims: 128 } }
      kv_policy: { cache: "latent", latent: { dim: 256, heads: 8, update: { kind: "linear", freq: 4, tau: 0.05 } } }
    memory_stage:
      kind: "transformer"
      d_model: 640
      n_layers: 1
      mix_unit:
        kind: "single"
        mixer: { kind: "Retention", heads: 4, chunk: 256, mode: "parallel" }
      ffn: { kind: "dense", mult: 2.0, act: "gelu" }
      norm: RMSNorm
      pos: { kind: "rope", rope: { theta: 10000, dims: 64 } }
    readout:
      kind: "readout"
      output_dim: 65536
  pipeline:
    - { name: embeddings, module: embeddings, kind: "embedding" }
    - { name: encoder, module: encoder_noncausal, mode: "train_prefill" }
    - { name: latent, module: latent_sampler, inputs: ["encoder"], mode: "train_prefill" }
    - { name: memory, module: memory_stage, inputs: ["encoder"], kv_from: ["encoder"] }
    - { name: decoder_lower, module: decoder_lower, kv_from: ["encoder", "memory"] }
    - { name: decoder_upper, module: decoder_upper, kv_from: ["decoder_lower", "latent", "memory"] }
    - { name: readout, module: readout, inputs: ["decoder_upper"] }
train:
  ctx_len: 8192
  dtype: fp16
  vocab_size: 65536
  budget: { tokens_per_step: 65536, max_steps: 600, flops_per_step: 1200000000000.0 }
